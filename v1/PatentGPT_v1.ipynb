{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PatentGPT-v1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2dMuHFhsQEC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a0e1a4ea-1e9e-42bc-f7fb-7a8810547ec3"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import pdb\n",
        "\n",
        "proj_folder = '/content/gpt-2'\n",
        "git_src = 'https://github.com/openai/gpt-2' \n",
        "if not os.path.exists(proj_folder):\n",
        "  !git clone $git_src\n",
        "else:\n",
        "  print('existed: %s' % proj_folder)\n",
        "  os.chdir(proj_folder)  \n",
        "  !git pull origin master\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 4.83 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n",
            "Collecting fire>=0.1.3\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "  Downloading regex-2017.04.05.tar.gz (601 kB)\n",
            "\u001b[K     |████████████████████████████████| 601 kB 13.1 MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "  Downloading requests-2.21.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "  Downloading tqdm-4.31.1-py2.py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2021.5.30)\n",
            "Collecting idna<2.9,>=2.5\n",
            "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Building wheels for collected packages: regex, fire\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534440 sha256=b1e6ee8e5c19ec21c178d025e7b739d24f174fcd7636fbf18fd02d3f34ed0c48\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/e8/a5/d4894e7ef29935f75c6074409ce8ca80a0271f0ce2a30da5d3\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115943 sha256=aa522308b30c60c2c03eb3ec6d548aec6edca2dcb7b9ee3b5291f7f00ac86185\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built regex fire\n",
            "Installing collected packages: idna, tqdm, requests, regex, fire\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.62.0\n",
            "    Uninstalling tqdm-4.62.0:\n",
            "      Successfully uninstalled tqdm-4.62.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 2.2.4 requires tqdm<5.0.0,>=4.38.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "panel 0.12.1 requires tqdm>=4.48.0, but you have tqdm 4.31.1 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.21.0 which is incompatible.\n",
            "fbprophet 0.7.1 requires tqdm>=4.36.1, but you have tqdm 4.31.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed fire-0.4.0 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUzg8NkJ16LS"
      },
      "source": [
        "This Colab notebook demonstrates how to generete  patent claims  based  on  an  fine-tuned  OpenAI  GPT-2  model.  \n",
        "\n",
        "This notebook will download related model files and create required source code automatically. \n",
        "\n",
        "Citation for this work: https://doi.org/10.1016/j.wpi.2020.101983 \n",
        "\n",
        "\n",
        "```\n",
        "@article{patent_bert, \n",
        "  author = \"Jieh-Sheng Lee and Jieh Hsiang\",\n",
        "  title = \"{Patent claim generation by fine-tuning OpenAI GPT-2}\",\n",
        "  journal = \"World Patent Information\",\n",
        "  volume = \"62\",\n",
        "  number = \"101983\",\n",
        "  year = \"2020\",\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZJwuWIJsQI_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0592e6e-dc98-42be-acc6-e679afe0b746"
      },
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "print('tf version: %s' % tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if 'GPU' in device_name:\n",
        "  print('GPU ready: %s' % device_name) \n",
        "  GPU_FLAG = True\n",
        "else:\n",
        "  print('CPU only.....')    \n",
        "\n",
        "src_path = '/content/gpt-2/src'\n",
        "if src_path not in sys.path:\n",
        "  sys.path += [src_path]\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "model_name= '345M'  \n",
        "if os.path.exists(os.path.join('models', model_name)) == False:\n",
        "  print('download model %s....' % model_name)\n",
        "  !PYTHONPATH=src; python ./download_model.py $model_name\n",
        "else:\n",
        "  print('existed: model %s' % model_name)    "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf version: 1.15.2\n",
            "GPU ready: /device:GPU:0\n",
            "download model 345M....\n",
            "Fetching checkpoint: 1.00kit [00:00, 516kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:01, 952kit/s]                                                    \n",
            "Fetching hparams.json: 1.00kit [00:00, 699kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [03:15, 7.25Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 6.72Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:01, 869kit/s]                                                  \n",
            "Fetching vocab.bpe: 457kit [00:00, 506kit/s]                                                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9C9lZ6jgrCo"
      },
      "source": [
        "# the following code is copied from: \n",
        "# https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    save_response_content(response, destination)    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI-rkoeYTWFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1745df2c-bd9b-4b73-e873-8b213ff3ac70"
      },
      "source": [
        "# donwload fine-tuned model for patents\n",
        "\n",
        "ckpt_path = 'saved_checkpoint'\n",
        "if os.path.exists(ckpt_path):\n",
        "  print('Existed: %s' % ckpt_path)\n",
        "  !ls $ckpt_path\n",
        "else:\n",
        "  os.mkdir(ckpt_path)\n",
        "  os.chdir(ckpt_path)\n",
        "  print('Downloading files to %s....' % ckpt_path)\n",
        "  download_file_from_google_drive('10Qc8SIuwq6w6guwDnpdNzKH24NxZ3oG-', 'checkpoint')\n",
        "  download_file_from_google_drive('1KDO8ikS5IJqo1S6zfbOnFae3-zS6-8f0', 'model-945000.meta')\n",
        "  download_file_from_google_drive('1Z0PnW0BHFApZBBWL3SGDzwgFme1mwFwi', 'model-945000.data-00000-of-00001')\n",
        "  download_file_from_google_drive('1ipOdKVOUM8h45njLLtbhJFbfqTNFLNq_', 'model-945000.index')\n",
        "  !ls -al \n",
        "  print('Download: ok')\n",
        "os.chdir(proj_folder)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading files to saved_checkpoint....\n",
            "total 1393592\n",
            "drwxr-xr-x 2 root root       4096 Sep 11 03:16 .\n",
            "drwxr-xr-x 6 root root       4096 Sep 11 03:16 ..\n",
            "-rw-r--r-- 1 root root        253 Sep 11 03:16 checkpoint\n",
            "-rw-r--r-- 1 root root 1419292672 Sep 11 03:16 model-945000.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      10399 Sep 11 03:16 model-945000.index\n",
            "-rw-r--r-- 1 root root    7715712 Sep 11 03:16 model-945000.meta\n",
            "Download: ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiBKgo1SsQWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5ebb647-dc21-46e6-fd4d-b30379bce18b"
      },
      "source": [
        "# the following is my enchancement based on: \n",
        "# https://github.com/openai/gpt-2/blob/master/src/sample.py\n",
        "# https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
        "\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import model, sample, encoder\n",
        "\n",
        "def generate_text(seed_text, sess, output, context, nsamples, batch_size):\n",
        "    generated = 0\n",
        "    for _ in range(nsamples // batch_size):\n",
        "      out = sess.run(output, feed_dict={\n",
        "          context: [context_tokens for _ in range(batch_size)]\n",
        "      })[:, len(context_tokens):]\n",
        "      for i in range(batch_size):\n",
        "        generated += 1\n",
        "        text = enc.decode(out[i])\n",
        "        text = till_end_of_text(text)\n",
        "        text = seed_text + ' ' + text\n",
        "        print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
        "        text = text.replace(tag_start, '').strip()\n",
        "        span_text = text.replace(' @@@ ','\\n    ')\n",
        "        print(span_text)\n",
        "    print(\"=\" * 80)\n",
        "    print('\\n')\n",
        "\n",
        "def dynamic_kp_logits(logits, top_kp):\n",
        "    k = 1000 # observe probability of the top n \n",
        "    probs_logits = tf.nn.softmax(logits)\n",
        "    k_probs, _ = tf.nn.top_k(probs_logits, k=k)\n",
        "    k_probs = tf.squeeze(k_probs)\n",
        "    probs_max = tf.reduce_max(k_probs)\n",
        "    k_threshold = tf.multiply(probs_max, top_kp)\n",
        "    probs_mask = tf.to_int32(k_probs >= k_threshold)\n",
        "    num_of_k = tf.count_nonzero(probs_mask, dtype=tf.float32) \n",
        "\n",
        "    values, _ = tf.nn.top_k(logits, k=tf.cast(num_of_k, dtype=tf.int32))\n",
        "    min_values = values[:, -1, tf.newaxis]\n",
        "    result_logits = tf.where(\n",
        "        logits < min_values,\n",
        "        tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
        "        logits,)\n",
        "\n",
        "    return result_logits\n",
        "\n",
        "def sample_sequence_kp(*, hparams, length, start_token=None, batch_size=None, \n",
        "                       context=None, temperature=1, top_kp=0.1):\n",
        "    if start_token is None:\n",
        "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
        "    else:\n",
        "        assert context is None, 'Specify exactly one of start_token and context!'\n",
        "        context = tf.fill([batch_size, 1], start_token)\n",
        "\n",
        "    def step(hparams, tokens, past=None):\n",
        "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
        "        presents = lm_output['present']\n",
        "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'presents': presents,\n",
        "        }\n",
        "\n",
        "    with tf.name_scope('sample_sequence'):\n",
        "        def body(past, prev, output):\n",
        "            next_outputs = step(hparams, prev, past=past)\n",
        "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
        "            logits = dynamic_kp_logits(logits, top_kp)\n",
        "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
        "            return [\n",
        "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
        "                samples,\n",
        "                tf.concat([output, samples], axis=1)\n",
        "            ]\n",
        "\n",
        "        past, prev, output = body(None, context, context)\n",
        "\n",
        "        def cond(*args):\n",
        "            return True\n",
        "\n",
        "        _, _, tokens = tf.while_loop(\n",
        "            cond=cond, body=body,\n",
        "            maximum_iterations=length - 1,\n",
        "            loop_vars=[\n",
        "                past,\n",
        "                prev,\n",
        "                output\n",
        "            ],\n",
        "            shape_invariants=[\n",
        "                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "                tf.TensorShape([batch_size, None]),\n",
        "            ],\n",
        "            back_prop=False,\n",
        "        )\n",
        "\n",
        "        return tokens\n",
        "\n",
        "def till_end_of_text(text):\n",
        "  tag_end = '<|endoftext|>'\n",
        "  result = ''\n",
        "  pos2 = text.find(tag_end)\n",
        "  if pos2 == -1:\n",
        "    result = '(unable to generate....probably not patent text)'\n",
        "  elif pos2 == 0:\n",
        "    result = '(end of patent claim)'\n",
        "  else:\n",
        "    result = text[:pos2].strip()\n",
        "\n",
        "  return result\n",
        "\n",
        "# main program\n",
        "model_name='345M'\n",
        "seed=None\n",
        "nsamples=1\n",
        "batch_size=1\n",
        "length=None\n",
        "temperature=1\n",
        "top_k=40\n",
        "top_p=0.9\n",
        "top_kp=0.1\n",
        "\n",
        "tag_start = '<|startoftext|>'\n",
        "\n",
        "models_dir = 'models'\n",
        "models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "if batch_size is None:\n",
        "  batch_size = 1\n",
        "assert nsamples % batch_size == 0\n",
        "\n",
        "enc = encoder.get_encoder(model_name, models_dir)\n",
        "hparams = model.default_hparams()\n",
        "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "  hparams.override_from_dict(json.load(f))\n",
        "\n",
        "if length is None:\n",
        "  length = hparams.n_ctx // 2\n",
        "elif length > hparams.n_ctx:\n",
        "  raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "with tf.Session(graph=tf.Graph()) as sess:\n",
        "  context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "  np.random.seed(seed)\n",
        "  tf.set_random_seed(seed)\n",
        "\n",
        "  # original sampling in GPT-2\n",
        "  output_top_k = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=top_k, top_p=1)\n",
        "  output_top_p = sample.sample_sequence(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_k=0, top_p=top_p)\n",
        "  \n",
        "  # a different sampling in our research\n",
        "  output_top_kp = sample_sequence_kp(\n",
        "    hparams=hparams, length=length,\n",
        "    context=context,\n",
        "    batch_size=batch_size,\n",
        "    temperature=temperature, top_kp=top_kp)\n",
        "\n",
        "  saver = tf.train.Saver()\n",
        "\n",
        "  # use the fine-tuned model for patents\n",
        "  ckpt = tf.train.latest_checkpoint(ckpt_path)\n",
        "\n",
        "  # original model released by OpenAI\n",
        "  # ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "\n",
        "  saver.restore(sess, ckpt)\n",
        "  while True:\n",
        "    print('Input text or \"exit\" or \"Enter\" key for unconditional sampling.....')\n",
        "    seed_text = input(\">>> \")\n",
        "    if seed_text == 'exit':\n",
        "      break\n",
        "    if seed_text == '':\n",
        "      seed_text = tag_start \n",
        "    context_tokens = enc.encode(seed_text)\n",
        "    print('top_k = %s' % top_k)\n",
        "    generate_text(seed_text, sess, output_top_k, context, nsamples, batch_size)\n",
        "    print('top_p = %s' % top_p)\n",
        "    generate_text(seed_text, sess, output_top_p, context, nsamples, batch_size)\n",
        "    print('top_kp = %s' % top_kp)\n",
        "    generate_text(seed_text, sess, output_top_kp, context, nsamples, batch_size)\n",
        "\n",
        "  print('Thank you.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From <ipython-input-5-e2174fb991e0>:36: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Restoring parameters from saved_checkpoint/model-945000\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n",
            ">>> A method for cooling\n",
            "top_k = 40\n",
            "======================================== SAMPLE 1 ========================================\n",
            "A method for cooling a solar array comprising at least one photovoltaic panel, the method comprising:\n",
            "    circulating liquid cooling medium in a continuous circulating loop, the circulating liquid cooling medium comprising an evaporator, the evaporator comprising a plurality of individual tanks for circulating fluid there-through, the individual tanks configured and dimensioned to contain a predetermined amount of a fluid cooling medium, the evaporator configured to regulate a temperature of each of the plurality of individual tanks to a predefinable set of limits, the set of limits for each temperature of each of the plurality of individual tanks, the set of limits based on a desired value for a predetermined fluid temperature threshold associated with each of the plurality of individual tanks;\n",
            "    circulating cooling medium from the predefinable set of limits into the plurality of individual tanks, the cooling medium circulates between the plurality of individual tanks while circulating at a rate controlled by the fluid temperature threshold of each of the plurality of individual tanks and by a setpoint temperature of the cooling medium; and\n",
            "    circulating the cooling medium from an external location external to the photovoltaic array to a cooling medium circulation path.\n",
            "================================================================================\n",
            "\n",
            "\n",
            "top_p = 0.9\n",
            "======================================== SAMPLE 1 ========================================\n",
            "A method for cooling through a gap between two spacers in a semiconductor wafer having a circuit layer overlying a device layer, comprising:\n",
            "    heating the semiconductor wafer to reach a first temperature corresponding to a device temperature for the semiconductor wafer;\n",
            "    forming the gap between the spacers to a second temperature, a first gap temperature of the gap being above the device temperature for the semiconductor wafer; and\n",
            "    deforming a portion of the semiconductor wafer from the device temperature to the first gap temperature.\n",
            "================================================================================\n",
            "\n",
            "\n",
            "top_kp = 0.1\n",
            "======================================== SAMPLE 1 ========================================\n",
            "A method for cooling a solar PV system having a plurality of solar modules, the method comprising:\n",
            "    separating the solar modules from the plurality of solar modules;\n",
            "    separating a first fluid stream from the solar modules into a coolant fluid and a feedstock fluid;\n",
            "    passing the feedstock fluid through a first heat exchanger to produce a first heated fluid;\n",
            "    passing the first heated fluid through a second heat exchanger to produce a second heated fluid;\n",
            "    passing the first heated fluid through a third heat exchanger to produce a coolant fluid; and\n",
            "    passing the coolant fluid through a fourth heat exchanger to produce a coolant fluid;\n",
            "    wherein the feedstock fluid is recirculated between the solar modules to cool the system.\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-ikNAbesQSn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}