
# PatentTransformer

PatentTransformer is our codename for "Augmented Inventing." The ultimate goal of this project is to help inventors conceive better inventions and quality patents. We leverage Transformer-based models, such as GPT-2 and BERT for patent text generation and measurement. Our source code will be released soon.

## Our preprints & papers

  * [Patent Claim Generation by Fine-Tuning OpenAI GPT-2](https://arxiv.org/abs/1907.02052) (under review)
  * [Measuring Patent Claim Generation by Span Relevancy](https://arxiv.org/abs/1908.09591) (To be published in the Proceedings of the Thirteenth International Workshop on Juris-informatics (JURISIN 2019), hosted by JSAI-isAI2019)
  * [Personalized Patent Claim Generation and Measurement](https://arxiv.org/abs/1912.03502) (Best Doctoral Consortium paper at the 32nd International Conference on Legal Knowledge and Information Systems (JURIX 2019). To be published in the CEUR Workshop Proceedings)
  * [Measuring and Controlling Text Generation by Semantic Search: A Textual Similarity Approach for PatentTransformer](https://www.researchgate.net/publication/338964985_Measuring_and_Controlling_Text_Generation_by_Semantic_Search_A_Textual_Similarity_Approach_for_PatentTransformer) (accepted, PhD Symposium of The Web Conference 2020 (formerly known as WWW conference))
  * [PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model](https://arxiv.org/abs/1906.02124) (under review)
  * [PatentTransformer-2: Controlling Patent Text Generation by Structural Metadata](https://arxiv.org/abs/2001.03708) (under review)

## License

[GNU General Public License v3.0](LICENSE)
