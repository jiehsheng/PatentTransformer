{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "(paper9) (dev, gdrive) PatentTransformer-v2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXILwCtWpXPT",
        "colab_type": "text"
      },
      "source": [
        "### Augmented Inventing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvQBHzpZ86xX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q49Cdbl5ES9d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "pretrained_model = 'M2' \n",
        "\n",
        "# M1: small model for 1976~2016\n",
        "# M2: medium model for 1976~2016\n",
        "# M3: small model for 2016\n",
        "# M4: medium model for 2016\n",
        "\n",
        "if pretrained_model in ['M1', 'M3']:\n",
        "  model_name= '124M'\n",
        "elif pretrained_model in ['M2', 'M4']:\n",
        "  model_name= '355M'\n",
        "else:\n",
        "  print('unknown mode: %s' % pretrained_model)\n",
        "  sys.exit(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2dMuHFhsQEC",
        "colab_type": "code",
        "outputId": "3e752425-9e4e-4340-8c20-ba935e4ab4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import os\n",
        "\n",
        "proj_folder = '/content/gpt-2'\n",
        "git_src = 'https://github.com/openai/gpt-2' \n",
        "if not os.path.exists(proj_folder):\n",
        "  !git clone $git_src\n",
        "else:\n",
        "  print('existed: %s' % proj_folder)\n",
        "  os.chdir(proj_folder)  \n",
        "  !git pull origin master\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "existed: /content/gpt-2\n",
            "From https://github.com/openai/gpt-2\n",
            " * branch            master     -> FETCH_HEAD\n",
            "Already up to date.\n",
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.2.1)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.11.28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZJwuWIJsQI_",
        "colab_type": "code",
        "outputId": "e607063a-45d4-49e9-e41d-45b81c38aef2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('tf version: %s' % tf.__version__)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if 'GPU' in device_name:\n",
        "  print('GPU ready: %s' % device_name) \n",
        "  GPU_FLAG = True\n",
        "else:\n",
        "  print('CPU only.....')    \n",
        "\n",
        "src_path = '/content/gpt-2/src'\n",
        "if src_path not in sys.path:\n",
        "  sys.path += [src_path]\n",
        "\n",
        "os.chdir(proj_folder)\n",
        "if os.path.exists(os.path.join('models', model_name)) == False:\n",
        "  print('download model %s....' % model_name)\n",
        "  !PYTHONPATH=src; python ./download_model.py $model_name\n",
        "else:\n",
        "  print('existed: model %s' % model_name)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "tf version: 1.15.0\n",
            "GPU ready: /device:GPU:0\n",
            "existed: model 355M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9C9lZ6jgrCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the following code is copied from: \n",
        "# https://stackoverflow.com/questions/25010369/wget-curl-large-file-from-google-drive/39225039#39225039\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    def get_confirm_token(response):\n",
        "        for key, value in response.cookies.items():\n",
        "            if key.startswith('download_warning'):\n",
        "                return value\n",
        "\n",
        "        return None\n",
        "\n",
        "    def save_response_content(response, destination):\n",
        "        CHUNK_SIZE = 32768\n",
        "\n",
        "        with open(destination, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk: # filter out keep-alive new chunks\n",
        "                    f.write(chunk)\n",
        "\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    save_response_content(response, destination)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI-rkoeYTWFP",
        "colab_type": "code",
        "outputId": "a4382a93-8399-48fc-caf0-9066bc27744f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# donwload fine-tuned model for patents\n",
        "\n",
        "download_links = {\n",
        "  'M1':{\n",
        "    'checkpoint': '1F6YxlYHy-0f0vmsEi5gOe91wLBCxk6Ud',\n",
        "    'model.ckpt-1000000.index': '1KqSe-N9sW2hZEY7-M7RSwU0_CfK1sSDI',\n",
        "    'model.ckpt-1000000.meta': '1F21mkrvf3bfwx-2YZlT46_qU3Vgva7NI',\n",
        "    'model.ckpt-1000000.data-00000-of-00001': '1N4AI0N930-3heAUP2f5_pjuZQYC62J4w'\n",
        "  },\n",
        "  'M2':{\n",
        "    'checkpoint': '1eqWbBY03soS2uurKYdbj1GEdOvCAtbw9',\n",
        "    'model.ckpt-1000000.index': '1X96DuunLroV7CP4GJJpMuWxqtP0HxH9n',\n",
        "    'model.ckpt-1000000.meta': '1FoabPlD9Ek4vuTer2FVG7TLZCrKmdW1B',\n",
        "    'model.ckpt-1000000.data-00000-of-00001': '1AhU1Cx2_hpzYk3Wrsz7G36qOXWvOJfbY'\n",
        "  },\n",
        "  'M3':{\n",
        "    'checkpoint': '1g8F5Ju4-6QYXeTa74BxS4w_dv38bBWwS',\n",
        "    'model.ckpt-1000000.index': '1MLeMzyNdDnxTWoFdb6eacAqrv-EH79Hh',\n",
        "    'model.ckpt-1000000.meta': '1vCSneihB3As9rF0X6LoWltb7GKHIZEML',\n",
        "    'model.ckpt-1000000.data-00000-of-00001': '1IbrnX1fvSpuMoYVsjrBnMNTYnnTkukso'\n",
        "  },\n",
        "  'M4':{\n",
        "    'checkpoint': '16pNZTOS-YMlGZGSapMuhduLwrrIPRD7E',\n",
        "    'model.ckpt-1000000.index': '1TFIKqujh1s6285ShaN7QjNgNfm62qMp4',\n",
        "    'model.ckpt-1000000.meta': '1jEFYMNVPgId70Hkuy1yXdnUMzTXwC7XY',\n",
        "    'model.ckpt-1000000.data-00000-of-00001': '1FzesRtEhz7cu1ZxCbwQRsUd8uFuFyWmH'\n",
        "  },\n",
        "}\n",
        "\n",
        "ckpt_path = 'saved_checkpoint_%s' % model_name\n",
        "if os.path.exists(ckpt_path):\n",
        "  print('Existed: %s' % ckpt_path)\n",
        "  !ls $ckpt_path\n",
        "else:\n",
        "  os.mkdir(ckpt_path)\n",
        "  os.chdir(ckpt_path)\n",
        "  print('Downloading files to %s....' % ckpt_path)\n",
        "  for k, v in download_links[pretrained_model].items():\n",
        "    download_file_from_google_drive(v, k)\n",
        "  !ls -al \n",
        "  print('Download: ok')\n",
        "os.chdir(proj_folder)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Existed: saved_checkpoint_355M\n",
            "checkpoint\t\t\t\tmodel.ckpt-1000000.index\n",
            "model.ckpt-1000000.data-00000-of-00001\tmodel.ckpt-1000000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiBKgo1SsQWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import model, sample, encoder\n",
        "\n",
        "foward_start_tags = {'title':'<|startoftitle|>', \\\n",
        "                     'abstract':'<|startofabstract|>', \\\n",
        "                     'claim': '<|startoftext|>', \\\n",
        "                     'dep': '<|startoftext|>'}\n",
        "foward_end_tags = {'title':'<|endoftitle|>', \\\n",
        "                   'abstract':'<|endofabstract|>', \\\n",
        "                   'claim': '<|endoftext|>', \\\n",
        "                   'dep': '<|startoftext|>'}\n",
        "backward_start_tags = {'title':'<|backwardtitlestart>', \\\n",
        "                     'abstract':'<|backwardabstractstart>', \\\n",
        "                     'claim': '<|startofbackward|>'}\n",
        "backward_end_tags = {'title':'<|backwardtitleend|>', \\\n",
        "                   'abstract':'<|backwardabstractend|>', \\\n",
        "                   'claim': '<|endofbackward|>'}\n",
        "\n",
        "# text2text mapping\n",
        "tag_title2abstract = '<|title2abstract|>'\n",
        "tag_abstract2title = '<|abstract2title|>'\n",
        "tag_abstract2claim = '<|abstract2claim|>'\n",
        "tag_claim2abstract = '<|claim2abstract|>'\n",
        "dep_separator = '<|dep|>'\n",
        "\n",
        "def generate_output(context, count, num_of_generation, sess, text, \n",
        "                       sampler, enc, batch_size, cut_tag):\n",
        "  results = []\n",
        "\n",
        "  # forward\n",
        "  text = text.strip()\n",
        "  context_tokens = enc.encode(text)\n",
        "\n",
        "  out = sess.run(sampler, feed_dict={\n",
        "      context: [context_tokens for _ in range(batch_size)]\n",
        "  })[:, len(context_tokens):]\n",
        "  \n",
        "  for i in range(batch_size):\n",
        "    text = enc.decode(out[i])\n",
        "    pos = text.find(cut_tag)\n",
        "    if pos >= 0:\n",
        "      text = text[:pos].strip()\n",
        "    if text == '':\n",
        "      continue\n",
        "      \n",
        "    results.append(text)\n",
        "    count += 1\n",
        "    if count >= num_of_generation:\n",
        "      break\n",
        "      \n",
        "  return results  \n",
        "\n",
        "def text2text_mapping(input_text, mapping, gen_count=1):\n",
        "  all_results = []\n",
        "  if mapping == 'dep':\n",
        "    meta1 = meta2 = 'claim'\n",
        "    print('[ dependent claim ]')\n",
        "  else:\n",
        "    meta1, meta2 = mapping.split('2')\n",
        "    print('[ %s --> %s ]' % (meta1, meta2))\n",
        "  raw_text = ''\n",
        "\n",
        "  count = 0 \n",
        "  raw_text = ' '.join([foward_start_tags[meta1], input_text, \\\n",
        "    foward_end_tags[meta1]]) \n",
        "  raw_text += ' <|' + mapping + '|> ' + foward_start_tags[meta2]\n",
        "  while count < gen_count:\n",
        "    batch_results = generate_output(context, count, \n",
        "      gen_count, sess, raw_text, sampler, enc, \n",
        "      batch_size, foward_end_tags[meta2])\n",
        "    count += len(batch_results)\n",
        "    all_results += batch_results\n",
        "\n",
        "  for i, row in enumerate(all_results):\n",
        "    row = row.replace('<|span|>', '\\n\\t')\n",
        "    print('%s' % row) \n",
        "    #print('[ %s ] %s' % (i, row))\n",
        "  print('')\n",
        "\n",
        "  return all_results\n",
        "\n",
        "def patent_text_gen(input_text, metadata, direction='forward', gen_count=1):\n",
        "  all_results = []\n",
        "\n",
        "  print('[ %s ] direction=%s, input_text=%s' % (metadata, direction, input_text))\n",
        "  count = 0 \n",
        "  if direction == 'forward':\n",
        "    raw_text = foward_start_tags[metadata] + ' ' + input_text\n",
        "    while count < gen_count:\n",
        "      batch_results = generate_output(context, count, \n",
        "        gen_count, sess, raw_text, sampler, enc, \n",
        "        batch_size, foward_end_tags[metadata])\n",
        "      count += len(batch_results)\n",
        "      for i, row in enumerate(batch_results):\n",
        "        s = input_text + ' ' + row\n",
        "        all_results.append(s.strip())\n",
        "  elif direction == 'backward':\n",
        "    reversed_text = ' '.join(input_text.split()[::-1])\n",
        "    raw_text = backward_end_tags[metadata] + ' ' + reversed_text\n",
        "    while count < gen_count:\n",
        "      batch_results = generate_output(context, count, \n",
        "        gen_count, sess, raw_text, sampler, enc, \n",
        "        batch_size, backward_start_tags[metadata])\n",
        "      count += len(batch_results)       \n",
        "      for i, row in enumerate(batch_results):\n",
        "        reversed_row = ' '.join(row.split()[::-1])\n",
        "        all_results.append(reversed_row + ' ' + input_text)\n",
        "  elif direction == 'both':\n",
        "    raw_text = foward_start_tags[metadata] + ' ' + input_text\n",
        "    # forward\n",
        "    while count < gen_count:\n",
        "      batch_results = generate_output(context, count, \n",
        "        gen_count, sess, raw_text, sampler, enc, \n",
        "        batch_size, foward_end_tags[metadata])\n",
        "      count += len(batch_results) \n",
        "      for i, row in enumerate(batch_results):\n",
        "        all_results.append(input_text + ' ' + row)\n",
        "\n",
        "    # backward, generate one by one\n",
        "    for i, one_record in enumerate(all_results):\n",
        "      reversed_text = ' '.join(one_record.split()[::-1])\n",
        "      raw_text = backward_end_tags[metadata] + ' ' + reversed_text\n",
        "      batch_results = generate_output(context, count, \n",
        "        1, sess, raw_text, sampler, enc, \n",
        "        batch_size, backward_start_tags[metadata])\n",
        "      reversed_result = ' '.join(batch_results[0].split()[::-1])\n",
        "      all_results[i] = reversed_result + ' ' + one_record\n",
        "  else: \n",
        "    print('unknown direction: %s' % direction)\n",
        "  \n",
        "  for i, row in enumerate(all_results):\n",
        "    print('%s' % row)\n",
        "    #print('[ %s ] %s' % (i, row))\n",
        "  print('')\n",
        "\n",
        "  return all_results  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMb9BXusYfrO",
        "colab_type": "code",
        "outputId": "fa32bd97-5917-4ebc-856c-6cae9dcf6f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# the following is my enchancement based on: \n",
        "# https://github.com/openai/gpt-2/blob/master/src/sample.py\n",
        "# https://github.com/openai/gpt-2/blob/master/src/interactive_conditional_samples.py\n",
        "\n",
        "# input_text: a few words \n",
        "# metadata: title / abstract / claim \n",
        "# text2text_mapping: title2abstract / abstract2title / abstract2claim /\n",
        "#                    claim2abstract\n",
        "# direction: forward / backward / both\n",
        "# gen_count: how many records to generate\n",
        "\n",
        "seed=None\n",
        "nsamples=1\n",
        "batch_size=1\n",
        "length=None\n",
        "temperature=1\n",
        "top_k=40\n",
        "\n",
        "models_dir = 'models'\n",
        "models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "if batch_size is None:\n",
        "  batch_size = 1\n",
        "assert nsamples % batch_size == 0\n",
        "\n",
        "enc = encoder.get_encoder(model_name, models_dir)\n",
        "hparams = model.default_hparams()\n",
        "with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "  hparams.override_from_dict(json.load(f))\n",
        "\n",
        "if length is None:\n",
        "  length = hparams.n_ctx // 2\n",
        "elif length > hparams.n_ctx:\n",
        "  raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "sess = tf.InteractiveSession() \n",
        "context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "sampler = sample.sample_sequence(\n",
        "  hparams=hparams, length=length,\n",
        "  context=context,\n",
        "  batch_size=batch_size,\n",
        "  temperature=temperature, top_k=top_k\n",
        ")\n",
        "saver = tf.train.Saver()\n",
        "ckpt = tf.train.latest_checkpoint(ckpt_path)\n",
        "saver.restore(sess, ckpt)\n",
        "\n",
        "#seed_text = 'temperature optimization'\n",
        "while True:\n",
        "  print('Demo: a few words --> title --> abstract --> independent claim --> dependent claims')\n",
        "  print('Input text or \"exit\" or \"Enter\" key for unconditional sampling.....')\n",
        "  seed_text = input(\">>> \")\n",
        "  direction = 'both'\n",
        "  if seed_text == 'exit':\n",
        "    break\n",
        "  if seed_text == '':\n",
        "    direction = 'forward'\n",
        "\n",
        "  # from a few words to a patent title\n",
        "  outputs = patent_text_gen(input_text=seed_text, metadata='title', \n",
        "                            direction=direction, gen_count=1)\n",
        "\n",
        "  # from the patent title to a patent abstract\n",
        "  results = text2text_mapping(input_text=outputs[0], mapping='title2abstract', gen_count=1)\n",
        "\n",
        "  # from the patent abstract to an independent claim\n",
        "  results = text2text_mapping(input_text=outputs[0], mapping='abstract2claim', gen_count=1)\n",
        "\n",
        "  # from the independent claim to two dependent claims\n",
        "  results = text2text_mapping(input_text=outputs[0], mapping='dep', gen_count=2)\n",
        "  \n",
        "print('Thank you for testing Augmented Inventing.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from saved_checkpoint_355M/model.ckpt-1000000\n",
            "Demo: a few words --> title --> abstract --> independent claim --> dependent claims\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n",
            ">>> image quality\n",
            "[ title ] direction=both, input_text=image quality\n",
            "Method and apparatus for image quality enhancement based on edge detection\n",
            "\n",
            "[ title --> abstract ]\n",
            "The present invention relates to a method and apparatus for increasing image quality in an image system such as a digital video image recording and retrieval system. In particular, in one embodiment, the present invention uses one or more edge detection algorithms to enhance image quality by identifying edges in the image or object and applying the algorithm(s) to the identified edges to correct for distortion in the image and thereby enhance image quality. In another embodiment, the present invention applies adaptive edge detection algorithms to identify edges, identify the number of detected edges to be used as a feature set, and applies an algorithm based on the set of candidate edge candidate features to correct for distortion in the image and thereby enhance image quality.\n",
            "\n",
            "[ abstract --> claim ]\n",
            "The method of enhancing image quality in images provided to a printer, said method comprising: \n",
            "\t determining an edge-detector threshold value for each pixel; \n",
            "\t detecting edge boundaries in a set of pixels in images provided to a printer; and \n",
            "\t controlling the application of a predetermined amount of enhancement to the pixel based on the detected edge boundaries for the pixels in the set, \n",
            "\t wherein the step of determining comprises: \n",
            "\t setting a set of reference pixels in a color space of the image; \n",
            "\t applying a color correction to the set of reference pixels to obtain a corrected set of reference pixels; \n",
            "\t comparing the edge-detector threshold value to each pixel in the corrected set of reference pixels to determine a pixel from the corrected set of reference pixels having an edge-detector value above the threshold value, the edge-detector value indicating whether the pixel represents edge; \n",
            "\t comparing the pixel of the set of reference pixels with a corresponding pixel in the color space of the image to determine a pixel value comprising the sum of the two comparison results; \n",
            "\t determining the difference between the pixel value and the edge-detection value for the pixel, wherein the step of determining a difference for each pixel comprises comparing it with a range of values in the color space of the image and when the pixel is outside the range of values in the color space of the image the pixel is assigned a smaller value.\n",
            "\n",
            "[ dependent claim ]\n",
            "The method according to claim 5, wherein at least one of said edge detection method and said apparatus are employed to perform image quality enhancement.\n",
            "The method of claim 10 wherein the step of applying a thresholding process to the data in a range of about 0.15% to about 0.25% in the frequency domain comprises the step of filtering the data in the frequency domain to detect a first transition of the data at a first edge, and \n",
            "\t wherein the step of comparing the first transition of the data with a data threshold to determine whether an edge exists at a first point of interest comprises the step of comparing the first transition of the data with a first threshold and a second transition of the data at an adjacent second point of interest to determine whether the first and second points of interest form a first edge.\n",
            "\n",
            "Demo: a few words --> title --> abstract --> independent claim --> dependent claims\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n",
            ">>> \n",
            "[ title ] direction=forward, input_text=\n",
            "Process for fabricating a flash memory device for forming buried contact holes\n",
            "\n",
            "[ title --> abstract ]\n",
            "The present invention discloses a process for fabricating a flash memory device, comprising the steps of: forming a first conductive type well and a first conductive type diffusion region in the substrate; forming a first gate oxide and a first polysilicon layer on the substrate; removing the first polysilicon layer and the first gate oxide and forming a first gate in the first conductive type well; forming a first insulating layer on the first gate and the first conductive type diffusion region; forming a second conducting layer on the first insulating layer; forming a second insulating layer on the second conducting layer; forming a third conducting layer on the second insulating layer; forming a third insulating layer on the third conducting layer; forming a plurality of openings in the third conducting layer, the first insulating layer, the second insulating layer and the third insulating layer to expose the second polysilicon layer and the first conductive type diffusion region; forming a conformal second polysilicon layer on the third insulating layer, the plurality of openings and covering the first polysilicon layer, the first gate oxide and the first conductive type diffusion region; removing all the third conducting layer, the second insulating layer and the third insulating layer; and forming first and second metal lines to connect each other over the exposed second polysilicon layer for forming a memory cell.\n",
            "\n",
            "[ abstract --> claim ]\n",
            "The method of fabricating a flash memory device comprising the steps of: \n",
            "\t providing a substrate having an insulating layer thereon; \n",
            "\t forming a plurality of gate electrodes in the insulate layer and spaced a predetermined distance apart from each other; \n",
            "\t forming a plurality of conductive lines parallel to the gate electrodes with an insulating layer formed between the gate electrodes; \n",
            "\t covering the substrate having the insulating layer, the gate electrodes, and \n",
            "\t the conductive lines with a passivation layer; \n",
            "\t removing a predetermined thickness of the passivation layer and the insulating layer using a mask; \n",
            "\t patterning the conductive lines using the passivation layer pattern as a mask and etching the insulating layer; \n",
            "\t thereafter, using a polysilicon as a blocking layer as a mask for exposing a predetermined area of the passivation layer and forming conductive blocks from the polysilicon; \n",
            "\t forming insulator spacers at lateral walls of the conductive blocks such that the conductive blocks are separated from each other; \n",
            "\t selectively forming insulator spacers for preventing the passivation layer exposed by the conductive blocks from being etched; and \n",
            "\t simultaneously forming buried contact holes by etching the passivation layer using the conductive blocks as etching masks and forming buried contact holes from the passivation layer using the conductive blocks as masking masks, such that the buried contact holes are vertically aligned with the conductive blocks and are in direct contact with the conductive lines.\n",
            "\n",
            "[ dependent claim ]\n",
            "Process according to claim 1, wherein the silicon oxide layer is deposited by plasma enhanced chemical vapor deposition at a pressure from 0.5 to 7.0 Pa (100 to 1, \n",
            "\t 000 Torr).\n",
            "Method of claim 1, wherein the process comprises forming a plurality of trenches, etching the sidewalls of the trenches with ion-beam etching, forming an amorphous silicon layer over the surfaces of the trenches, and \n",
            "\t depositing a doped polysilicon layer to fill the trenches.\n",
            "\n",
            "Demo: a few words --> title --> abstract --> independent claim --> dependent claims\n",
            "Input text or \"exit\" or \"Enter\" key for unconditional sampling.....\n",
            ">>> exit\n",
            "Thank you for testing Augmented Inventing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwPMFKbggieR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}